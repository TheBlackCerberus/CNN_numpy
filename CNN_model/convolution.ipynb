{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from functools import cache\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "class Convolution:\n",
    "    def __init__(self, filter_shape=(int, int), stride=int, padding='valid', pad=int):\n",
    "        self.hparams = {\n",
    "            \"filter_shape\" : filter_shape,\n",
    "            \"stride\" : stride,\n",
    "            \"pad\" : pad\n",
    "        }\n",
    "        self.cache = {}\n",
    "        self.padding = padding\n",
    "        self.gradients = {}\n",
    "\n",
    "\n",
    "    def single_convolution(self, input_slice, weights, bias):\n",
    "        \"\"\"\n",
    "\n",
    "        :param input_slice: shape => (f, f, dim_channels_prev)\n",
    "        :param weights: shape => (f, f, dim_channels_prev)\n",
    "        :param bias: shape => (1,1,1)\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        scalar = np.sum(np.multiply(input_slice, weights) + bias)\n",
    "        return scalar\n",
    "\n",
    "    def pad_by_zero(self, input_layer):\n",
    "        # pad input layer with the shape dim_train, dim_height_prev, dim_width_prev, dim_channels_prev\n",
    "        pad = self.hparams[\"pad\"]\n",
    "        return np.pad((0,0),(pad,pad),(pad, pad),(0,0), 'constant', constant_values=(0,0))\n",
    "\n",
    "\n",
    "    def conv_forward_pass(self, out_activation, weights, biases):\n",
    "\n",
    "        (dim_train, dim_height_prev, dim_width_prev, dim_channels_prev) = out_activation.shape\n",
    "        (f, f, dim_channels_prev, dim_channels) = weights.shape\n",
    "\n",
    "        # select the type of padding\n",
    "        if self.padding == \"same\":\n",
    "            dim_height = int((dim_height_prev - f + 2 * self.hparams[\"pad\"]) / self.hparams[\"stride\"]) + 1\n",
    "            dim_width = int((dim_width_prev - f + 2 * self.hparams[\"pad\"]) / self.hparams[\"stride\"]) + 1\n",
    "        elif self.padding == \"valid\":\n",
    "            dim_height = int(dim_height_prev - f + 1)\n",
    "            dim_width = int(dim_width_prev - f + 1)\n",
    "        else:\n",
    "            dim_height = int((dim_height_prev - f + 2 * self.hparams[\"pad\"]) / self.hparams[\"stride\"]) + 1\n",
    "            dim_width = int((dim_width_prev - f + 2 * self.hparams[\"pad\"]) / self.hparams[\"stride\"]) + 1\n",
    "\n",
    "        out_layer = np.zeros((dim_train, dim_height,  dim_width, dim_channels))\n",
    "        out_prev_A_pad = self.pad_by_zero(out_activation)\n",
    "\n",
    "        for i in range(dim_train):\n",
    "            out_prev_a = out_prev_A_pad[i,...]\n",
    "            for h in range(dim_height):\n",
    "                for w in range(dim_width):\n",
    "                    for c in range(dim_channels):\n",
    "                        height_start = h*self.hparams[\"stride\"]\n",
    "                        height_end = h*self.hparams[\"stride\"] + f\n",
    "                        width_start = w*self.hparams[\"stride\"]\n",
    "                        width_end = w*self.hparams[\"stride\"] + f\n",
    "\n",
    "                        out_slice_prev_a = out_prev_a[height_start:height_end, width_start:width_end,:]\n",
    "                        out_layer[i,h, w, c] = self.single_convolution(out_slice_prev_a, self.hparams[...,c], self.hparams[...,c])\n",
    "\n",
    "        assert out_layer.shape == (dim_train, dim_height,  dim_width, dim_channels)\n",
    "        self.cache[\"prev_activation\"] = out_activation\n",
    "        return out_layer\n",
    "\n",
    "    def initialise_cache(self):\n",
    "        cache = dict()\n",
    "        cache['dW'] = np.zeros_like(self.hparams['W'])\n",
    "        cache['db'] = np.zeros_like(self.hparams['b'])\n",
    "        return cache\n",
    "\n",
    "    def conv_backward_pass(self, dZ):\n",
    "        \"\"\"\n",
    "\n",
    "        :param dZ: gradient with respect to output of CNN layer => shape (dim_train, dim_height, dim_width, dim_channels)\n",
    "        :return: return gradient with respect to previous CNN layer\n",
    "        \"\"\"\n",
    "\n",
    "        prev_activation = self.cache(\"prev_activation\")\n",
    "        (dim_train, dim_height_prev, dim_height_prev, dim_channels_prev) = prev_activation.shape\n",
    "        (f, f) = self.hparams[\"kernel_shape\"]\n",
    "        pad = self.hparams[\"pad\"]\n",
    "\n",
    "        (dim_train, dim_height, dim_width, dim_channels) = dZ.shape\n",
    "\n",
    "        dA_prev = np.zeros((dim_train, dim_height_prev, dim_height_prev, dim_channels_prev))\n",
    "        self.gradients = self.initialise_cache()\n",
    "\n",
    "        A_prev_pad = self.pad_by_zero(prev_activation)\n",
    "        dA_prev_pad = self.pad_by_zero(dA_prev)\n",
    "\n",
    "        for i in range(dim_train):\n",
    "\n",
    "            for h in range(dim_height):\n",
    "                a_prev_pad = A_prev_pad[i,...]\n",
    "                da_prev_pad = dA_prev_pad[i,...]\n",
    "                for w in range(dim_width):\n",
    "                    for c in range(dim_channels):\n",
    "                        height_start = h*self.hparams[\"stride\"]\n",
    "                        height_end = h*self.hparams[\"stride\"] + f\n",
    "                        width_start = w*self.hparams[\"stride\"]\n",
    "                        width_end = w*self.hparams[\"stride\"] + f\n",
    "\n",
    "                        a_slice = a_prev_pad[height_start:height_end, width_start:width_end, :]\n",
    "                        da_prev_pad[height_start:height_end, width_start:width_end, :] += self.hparams[\"W\"][:,:,:,c] * dZ[i,h,w,c]\n",
    "                        self.hparams[\"W\"][:,:,:, c] += a_slice * dZ[i,h,w,c]\n",
    "                        self.hparams[:,:,:,c] += dZ[i, h, w, c]\n",
    "\n",
    "            dA_prev[i, :, :, :] = da_prev_pad[pad:-pad, pad:-pad, :]\n",
    "\n",
    "        assert(dA_prev.shape == (dim_train, dim_height_prev, dim_height_prev, dim_channels_prev))\n",
    "        return dA_prev\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}